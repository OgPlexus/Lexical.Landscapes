{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadng the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from scipy.integrate import odeint\n",
    "import pandas as pd\n",
    "import pylab as p\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\"\"\" WL is the word-length \"\"\"\n",
    "WL = 4   \n",
    "\n",
    "os.chdir('C:/Users/Miles/Documents/Brown/Ogbunu Lab/Evo Word') # sets the cwd to preferred directory \n",
    "\n",
    "file_name = '%s_Letter_OneGrams_1900_2000.csv'%WL   # for 4 letter words; the data was ordered from most popular words to least\n",
    "\n",
    "\"\"\" defines a list called 'data' which holds the ngram scores for various years \"\"\"\n",
    "with open(file_name, 'r') as f:        \n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)                                        \n",
    "    \n",
    "for i in range(1,len(data)):\n",
    "    data[i][1:] = map(int,data[i][1:])         # the above makes all elements strings, so this turns word counts to ints\n",
    "    \n",
    "ngram_words = []  \n",
    "for i in range(1,len(data)):\n",
    "    ngram_words.append(data[i][0])             # words from google's ngram\n",
    "    \n",
    "    \n",
    "\"\"\" this identifies the most popular words, using our 97% cutoff criterion \"\"\"\n",
    "\n",
    "cutoff = [201,1501,5001][WL-3]\n",
    "pop_words = [data[i][0] for i in range(1,cutoff)]   # this houses the popular words\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-pair Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" A word-pair is a pair of words with the same length where you can go from one to the other by way of swapping \n",
    "    a letter at a time from one word to the other, such that each intermediate letter combination is also a word. A 4-letter\n",
    "    example is WORD to GENE, since there is the path: WORD -> WORE -> GORE -> GONE -> GENE. Here, we restrict our \n",
    "    attention to pairs of words that do not share letters in the same respective location (so that the no. of steps in \n",
    "    any path is the same as the length of the words in the pair). \n",
    "\"\"\"\n",
    "\n",
    "\"\"\" functions \"\"\"\n",
    "def like_words(X,letter):                      # constructs a lst of words like X but with a different letter (@ 'letter') \n",
    "    Y = X[:letter]+X[letter+1:]\n",
    "    sims = []\n",
    "    for each in pop_words:\n",
    "        test = each[:letter]+each[letter+1:]\n",
    "        if test == Y:\n",
    "            sims.append(each)\n",
    "    sims.remove(X)\n",
    "    return sims\n",
    "\n",
    "def like_words_in_set(X,letter,Set):            # gives all words (in set) off by the given letter (=int giving position)\n",
    "    Y = X[:letter]+X[letter+1:]\n",
    "    sims = []\n",
    "    for each in Set:\n",
    "        test = each[:letter]+each[letter+1:]\n",
    "        if test == Y:\n",
    "            sims.append(each)\n",
    "    sims.remove(X)\n",
    "    return sims\n",
    "\n",
    "def new_word(X,letter):                         # picks a random word from like_words (if no like word then returns 0)\n",
    "    test_words = like_words(X,letter)\n",
    "    if len(test_words)>0:\n",
    "        new = np.random.choice(test_words)\n",
    "        return new\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def all_like_words(X):                          # gives a list of all words one letter removed from the input (in pop_words)\n",
    "    all_words = [] \n",
    "    for j in range(len(X)):\n",
    "        all_words.extend(like_words(X,j))\n",
    "    return all_words    \n",
    "\n",
    "def all_like_words_in_set(X,Set):                   # gives a list of all words one letter removed from the input (in given set)\n",
    "    all_words = [] \n",
    "    for j in range(len(X)):\n",
    "        all_words.extend(like_words_in_set(X,j,Set))\n",
    "    return all_words    \n",
    "\n",
    "\n",
    "N = 100         # number of word pairs to find\n",
    "i = 0           # serves as an index to be called in the constructor\n",
    "Da_Words = []   # will house a list of word-pairs, each having at least one word-path \n",
    "\n",
    "\"\"\" \n",
    "In a nutshell, the algorithm below identifies N word-pairs with paths among each                                  \n",
    "\n",
    " Step 1: find a random word                                                                                        \n",
    "\n",
    " Step 2: find another random word, the same as the first, but with one letter randomly swapped to a new letter     \n",
    "\n",
    " Step 3: repeat Step 2 recursively (using the previous word as the start of the new iteration), making sure to     \n",
    "         swap different letter locations each time, until either no word can be found or until all letters have    \n",
    "         been swaped to new ones. If all words are found, the start and ending words form the word-pair.\n",
    "         If no new word can be found at any step, begin again at Step 1.                                         \n",
    "\n",
    "Repeat Steps 1-3 until N word-pairs have been identified                                                \n",
    "\"\"\"\n",
    "\n",
    "while i < N:\n",
    "    condition = 0                          # initializes a break condition in the second while loop\n",
    "    rnge = list(range(WL))\n",
    "    word1 = np.random.choice(pop_words)    # finds a random test candidate called word1\n",
    "    Da_Words.append([word1])\n",
    "    r = np.random.choice(rnge)             # selects a random letter\n",
    "    rnge.remove(r)\n",
    "    word2 = new_word(word1,r)              # selects a new random word, word2, like word1 but with a new letter at location r \n",
    "    i += 1\n",
    "    while condition == 0:\n",
    "        if word2 == 0:                     # if no such word2 could be found:\n",
    "            Da_Words.remove(Da_Words[i-1])\n",
    "            condition = 1                  # breaks the while loop\n",
    "            i -= 1\n",
    "            continue\n",
    "        if len(rnge)==0:                   # if the algorithm reaches the last letter:\n",
    "            Da_Words[i-1].append(word2)\n",
    "            condition = 1                  # breaks the loop \n",
    "        else:\n",
    "            r = np.random.choice(rnge)\n",
    "            rnge.remove(r)\n",
    "            word2 = new_word(word2,r)\n",
    "\n",
    "           \n",
    "        \n",
    "\"\"\" can also choose your own word pairs, so long as a path exists between them (turn off to keep the previous 'Da_Words') \"\"\"        \n",
    "Da_Words = [['WORD','GENE']]    # this is an example among 4-letter words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A word chain is the set of all letter combinations between two words in a given word-pair (i.e. there will be 2**WL \n",
    "    such combinations, where WL is the word length)\n",
    "\"\"\"\n",
    "\n",
    "bit_format = '{0:0%sb}'%str(WL)\n",
    "\n",
    "bits = []                       # a list of bit strings, numerically ordered, such as ['000','001','010',...]\n",
    "for i in range(2**WL):             \n",
    "    bits.append(bit_format.format(i))\n",
    "\n",
    "\"\"\" for each bit in 'bits', we collect the locations of the ones into a list (e.g. '1101'->[0,1,3]); these lists are collected\n",
    "    into another list we call 'indices'\n",
    "\"\"\"\n",
    "indices = []     \n",
    "\n",
    "for each in bits:\n",
    "    b = list(each)\n",
    "    indices.append([n for (n, e) in enumerate(b) if e == '1'])\n",
    "    \n",
    "\n",
    "word_chain = []\n",
    "\n",
    "for j in range(len(Da_Words)):\n",
    "    wordA = Da_Words[j][0]\n",
    "    wordB = Da_Words[j][1]\n",
    "    word_chain.append([wordA])\n",
    "    i = 1\n",
    "    while i<len(bits):\n",
    "        word = list(wordA)\n",
    "        for each in indices[i]:\n",
    "            word[each] = wordB[each]\n",
    "        word = ''.join(word)\n",
    "        word_chain[j].append(word)\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the array of fitness values, $W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" W[i][j][k] represents the fitness value of the kth word in the jth year in the ith word-pair; that is, the first index \n",
    "    indicates the word-pair (among N such pairs), the second index is the year, and the third index is the word in the \n",
    "    word-chain associated with the word-pair.\n",
    "\"\"\"\n",
    "\n",
    "W = []\n",
    "for i in range(len(Da_Words)):                            # this indexes over the pairs of words\n",
    "    W.append([])\n",
    "    for j in range(len(data[i][1:])):                     # this indexes over the years\n",
    "        W[i].append([])\n",
    "        for those in word_chain[i]:                       # this indexes over those in the word chain\n",
    "            if those in ngram_words:\n",
    "                index = 1 + ngram_words.index(those)      # we add 1 bc ngram_words is an index below words_from_google\n",
    "                W[i][j].append(data[index][j+1])\n",
    "            else:\n",
    "                W[i][j].append(0)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated Epistasis Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.linalg import hadamard\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "\n",
    "year = data[0][1:]              # string of the years\n",
    "\n",
    "order = ['$0^{th}$','$1^{st}$','$2^{nd}$','$3^{rd}$','$4^{th}$','$5^{th}$'][:(WL+1)]  # labels\n",
    "\n",
    "tick_spacing = 2                # sets how frequently years are displayed on the time axis\n",
    "\n",
    "H = hadamard(len(bits))         # Hadamard matrix of the appropriate size\n",
    "         \n",
    "v = np.array([[0.5,0],[0,-1]])  # this initializes the diagonal matrix V_1\n",
    "V = v\n",
    "\n",
    "for i in range(WL-1):\n",
    "    V = np.kron(V,v)\n",
    "\n",
    "M = np.matmul(V,H)                 # here we multiply the diagonal matrix V with H\n",
    "E = []\n",
    "\n",
    "for j in range(len(W)):\n",
    "    E.append([])\n",
    "    for i in range(len(year)):\n",
    "        E[j].append(np.array(M.dot(W[j][i])))\n",
    "\n",
    "\n",
    "# definfing the mean absolute epistasis arrays\n",
    "\n",
    "eabs = []\n",
    "orders = list(map(len,indices))   # this gives a list of the order (weight) of each bit string\n",
    "\n",
    "pascal = [[1,3,3,1],[1,4,6,4,1],[1,5,10,10,5,1]][WL-3]\n",
    "\n",
    "\n",
    "# this constructs the eabs array: a collection of averages of absolute values of elements in E (for each year and word chain)\n",
    "\n",
    "for k in range(len(W)):\n",
    "    bins = [[[],[],[],[]],\n",
    "            [[],[],[],[],[]],\n",
    "            [[],[],[],[],[],[]]][WL-3]\n",
    "    eabs.append(bins)\n",
    "\n",
    "    for each in E[k]:\n",
    "        s = sum(list(map(abs,each)))            # the line below to turns off normalization\n",
    "#         s = 1                                   # this shuts off the normalization of the epistasis\n",
    "        for o in range(len(order)):\n",
    "            epi_sum = sum(list(map(abs,[each[n] for (n,i) in enumerate(orders) if i==o]))) # sum of the abs(orders)\n",
    "            eabs[k][o].append(epi_sum/(pascal[o] * s))\n",
    "\n",
    "            \n",
    "            \n",
    "# plotting the mean abs epistasis over time\n",
    "\n",
    "for j in range(len(W)):\n",
    "    \n",
    "    fig,ax = p.subplots(figsize=[10,6])\n",
    "    \n",
    "    for i in range(len(order)):\n",
    "        ax.plot(year,eabs[j][i],label=order[i])\n",
    "\n",
    "    p.xlabel('Year',fontsize=20)\n",
    "    p.ylabel('Epistasis (Mean Absolute Value)',fontsize=20)\n",
    "    p.xticks(rotation=75)\n",
    "    p.tick_params(labelsize=15,axis='x',direction='in',top=1,right=1)\n",
    "    p.tick_params(labelsize=15,axis='y',direction='in',top=1,right=1)\n",
    "    p.title('Epistasis (Absolute Mean) in %s to %s' %(Da_Words[j][0],Da_Words[j][1]),fontsize=30)\n",
    "    p.legend(loc='center left',bbox_to_anchor=(1,0.5),fontsize=15)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "    \n",
    "    p.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaggregated Epistasis Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "year = data[0][1:]              # string of the years\n",
    "\n",
    "e_disag = []\n",
    "for j in range(len(W)):\n",
    "    e_disag.append([])\n",
    "    for o in range(len(order)):\n",
    "        e_disag[j].append([])\n",
    "        index = [n for (n,i) in enumerate(orders) if i==o]\n",
    "        for x in range(len(index)):\n",
    "            e_disag[j][o].append([])\n",
    "            for each in E[j]:\n",
    "                s = sum(list(map(abs,each)))   \n",
    "                e_disag[j][o][x].append(abs(each[index[x]])/s)\n",
    "                \n",
    "                \n",
    "                \n",
    "order_index = [[n for (n,i) in enumerate(orders) if i==o] for o in range(len(order))]\n",
    "pascal = [[1,3,3,1],[1,4,6,4,1],[1,5,10,10,5,1]][WL-3]\n",
    "\n",
    "\n",
    "colors = ['Purples', 'Reds', 'Greens', 'Blues', 'Oranges',\n",
    "            'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "            'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "\n",
    "labels = [[bits[i] for i in order_index[j]] for j in range(WL+1)]\n",
    "\n",
    "\n",
    "\"\"\" Here one can choose a word pair of interest; below we chose WORD to GENE as an example \"\"\"\n",
    "word = ['WORD','GENE']\n",
    "word_index = Da_Words.index(word)\n",
    "\n",
    "\n",
    "fig,ax = p.subplots(figsize=[10,6])\n",
    "\n",
    "for j in range(len(order)):\n",
    "    \n",
    "    cmap = mpl.cm.get_cmap(colors[j])\n",
    "    color = np.linspace(0.5,1,len(order_index[j]))\n",
    "    \n",
    "    for i in range(pascal[j]):\n",
    "        ax.plot(year,e_disag[word_index][j][i],label=labels[j][i],color=cmap(color[i])[:-1])\n",
    "\n",
    "p.xlabel('Year',fontsize=20)\n",
    "p.ylabel('Epistasis (Absolute Value)',fontsize=20)\n",
    "p.xticks(rotation=75)\n",
    "p.tick_params(labelsize=15,axis='x',direction='in',top=1,right=1)\n",
    "p.tick_params(labelsize=15,axis='y',direction='in',top=1,right=1)\n",
    "p.title('Epistasis in %s to %s (Absolute Mean)' %(word_chain[word_index][0],word_chain[word_index][-1]),fontsize=30)\n",
    "p.legend(loc='center left',bbox_to_anchor=(1,0.5),fontsize=15)\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(2))\n",
    "\n",
    "p.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing $W[j]$ and $E[j]$ to .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Select word-pair of interest; below we use WORD to GENE as an example. Structure save directory in preferred manner \"\"\"\n",
    "\n",
    "word = ['WORD','GENE']\n",
    "j = Da_Words.index(word)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\"\"\" fitness values \"\"\"\n",
    "\n",
    "np.savetxt('Fitness/English/%s Letter/fitness %s to %s.csv' %(str(WL),word[0],word[1]),[],delimiter=',')\n",
    "    \n",
    "with open('Fitness/English/%s Letter/fitness %s to %s.csv' %(str(WL),word[0],word[1]),'w') as file:\n",
    "    writer = csv.writer(file,delimiter=',')\n",
    "    for line in W[j]:\n",
    "        writer.writerow(line)\n",
    "        \n",
    "#------------------------------------------------------------------------------------------------------------#        \n",
    "\n",
    "\"\"\" epistasis values \"\"\"\n",
    "        \n",
    "np.savetxt('Epistasis/Epistasis Data/English/%s Letters/Epistasis %s to %s.csv' %(str(WL),word[0],word[1]),[],delimiter=',')\n",
    "    \n",
    "with open('Epistasis/Epistasis Data/English/%s Letters/Epistasis %s to %s.csv' %(str(WL),word[0],word[1]),'w') as file:\n",
    "    writer = csv.writer(file,delimiter=',')\n",
    "    for line in E[j]:\n",
    "        writer.writerow(line)\n",
    "        \n",
    "#------------------------------------------------------------------------------------------------------------#        \n",
    "        \n",
    "\"\"\" absolute mean epistasis values \"\"\"\n",
    "        \n",
    "np.savetxt('Epistasis/Absolute Mean Data/English/%s Letters/absolute Epistasis %s to %s.csv' %(str(WL),word[0],word[1]),[],delimiter=',')\n",
    "    \n",
    "with open('Epistasis/Absolute Mean Data/English/%s Letters/absolute Epistasis %s to %s.csv' %(str(WL),word[0],word[1]),'w') as file:\n",
    "    writer = csv.writer(file,delimiter=',')\n",
    "    for line in np.array(eabs[j]).T:\n",
    "        writer.writerow(line)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetworkX (fitness landscape): preliminary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizing the bits for the graph\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "def weight(bit):\n",
    "    count = 0\n",
    "    for each in list(bit):\n",
    "        if each=='1':\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "for each in bits:\n",
    "    b = list(each)\n",
    "    indices.append([n for (n, e) in enumerate(b) if e == '1'])\n",
    "    \n",
    "def orgnze_pos(indx): # this function creates a position dictionary for the landscape at word_chain[indx]   \n",
    "    orgnze = {}\n",
    "    for i in range(len(bits)):\n",
    "        bit = bits[i]\n",
    "        word = word_chain[indx][i]\n",
    "        orgnze.update({bit:word})\n",
    "\n",
    "    orgnzd_bits = [] \n",
    "    for wght in range(WL+1):    # wght should be in range(L+1) where L is the number of letters    \n",
    "        to_arrnge = []\n",
    "        wghts = [b for b in bits if weight(b)==wght]\n",
    "        for each in wghts:\n",
    "            to_arrnge.append(int(each,2))\n",
    "        to_arrnge = sorted(to_arrnge)\n",
    "        arrnged = []\n",
    "        for each in to_arrnge:\n",
    "            arrnged.append(('{0:0%sb}'%str(WL)).format(each))   # change to '{0:0Lb}' as needed\n",
    "        orgnzd_bits.extend(arrnged)\n",
    "\n",
    "    pascal = [[1,3,3,1],[1,4,6,4,1],[1,5,10,10,5,1]][WL-3]      # change to appropriate pascal array\n",
    "    coords = []\n",
    "    for i in range(len(pascal)):\n",
    "        p = pascal[i]\n",
    "        for j in range(p):\n",
    "            x = np.array([-1 + 2*(i+1)/(WL+2),1-(j+1)/(p+1)]) # change to fit within appropriate frame (need only change x-coordinate)\n",
    "            coords.append(x)\n",
    "\n",
    "    positions = {}\n",
    "    k = 0\n",
    "    for bit in orgnzd_bits:\n",
    "        positions.update({orgnze[bit]:coords[k]})\n",
    "        k+=1\n",
    "        \n",
    "    return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetworkX (fitness landscape): drawing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# graphing the network\n",
    "\n",
    "\"\"\" adjust the size of the node and words inside nodes below to accomodate a good fit \"\"\"\n",
    "\n",
    "NODE_SIZE = 2500  # 5200 for WL = 3, 2500 for WL = 4, 1000 for WL = 5\n",
    "FONT_SIZE = 15    # 25 for WL = 3, 15 for WL = 4, 8 for WL = 5\n",
    "\n",
    "\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "def dict2nx(aDictGraph):\n",
    "    \"\"\" Converts the given dictionary representation of a graph, \n",
    "    aDictGraph, into a networkx DiGraph (directed graph) representation.   \n",
    "    aDictGraph is a dictionary that maps nodes to its \n",
    "    neighbors (successors):  {node:[nodes]}\n",
    "    A DiGraph object is returned.\n",
    "    \"\"\"\n",
    "    g = nx.Graph()\n",
    "    for node, neighbors in aDictGraph.items():\n",
    "        g.add_node(node)  # in case there are nodes with no edges\n",
    "        for neighbor in neighbors:\n",
    "            g.add_edge(node, neighbor)\n",
    "    return g \n",
    "\n",
    "yrs = ['1900','1950','2000']       # choose years of interest\n",
    "sim_words = [['WORD','GENE']]\n",
    "chain_index = [0]   # index in the word chain for the pairs in sim_words\n",
    "\n",
    "for pair,index in zip(Da_Words,chain_index):\n",
    "    temp = {}            # this initializes the dictionary of the pair graph (for now it is a temporary variable)\n",
    "    for word in word_chain[index]:\n",
    "        linkd_to = all_like_words_in_set(word,word_chain[index])\n",
    "        temp.update({word:linkd_to})\n",
    "    for year in yrs:\n",
    "        yr = data[0].index(year)\n",
    "\n",
    "        fig,ax = plt.subplots(figsize=[12,6])  # change size as needed\n",
    "        graph = dict2nx(temp)\n",
    "\n",
    "\n",
    "        # node weights\n",
    "        node_weights = {}\n",
    "        for node in graph.nodes():\n",
    "            if node in ngram_words:\n",
    "                i = 1 + ngram_words.index(node)\n",
    "                wght = data[i][yr]\n",
    "                node_weights.update({node:wght})\n",
    "            else:\n",
    "                node_weights.update({node:0})\n",
    "                \n",
    "        n_weights = list(node_weights.values())\n",
    "        avg_wght = np.max(n_weights)\n",
    "        n_weights = [x/avg_wght for x in n_weights]   \n",
    "        \n",
    "\n",
    "        # edge weights\n",
    "        edge_weights = {}\n",
    "        ticker = 0\n",
    "        for edge in graph.edges():\n",
    "            ticker += 1\n",
    "            if edge[0] in ngram_words and edge[1] in ngram_words:\n",
    "                i = 1 + ngram_words.index(edge[0])\n",
    "                j = 1 + ngram_words.index(edge[1])\n",
    "                wght = abs(data[i][yr]-data[j][yr])\n",
    "                edge_weights.update({edge:wght})\n",
    "            else:\n",
    "                in_ngrams = [items for items in edge if items in ngram_words]\n",
    "                if len(in_ngrams)==1:\n",
    "                    wght = data[ngram_words.index(in_ngrams[0])][yr]\n",
    "                    edge_weights.update({edge:wght})\n",
    "                else:\n",
    "                    wght = 0\n",
    "                    edge_weights.update({edge:wght})\n",
    "\n",
    "        e_weights = list(edge_weights.values())\n",
    "        avg_wght = 3*np.mean(e_weights)   \"\"\" <--- Can adjust multiplicative factor to change edge thickness \"\"\"\n",
    "        e_weights = [x/avg_wght for x in e_weights]   \n",
    "\n",
    "\n",
    "        pos = orgnze_pos(index)\n",
    "\n",
    "        draw_edges = nx.draw_networkx_edges(graph,pos=pos,alpha=1,ax=None,width=e_weights)\n",
    "        draw_nodes = nx.draw_networkx_nodes(graph,pos=pos,ax=None,node_size=NODE_SIZE,alpha=1,linewidths=1,node_color=n_weights,cmap=plt.cm.Blues)\n",
    "        nx.draw_networkx_labels(graph,pos=pos,font_family='STIXGeneral',font_size=FONT_SIZE,font_weight='bold')\n",
    "        draw_nodes.set_edgecolor('k')\n",
    "        \n",
    "        cbar = fig.colorbar(draw_nodes,ax=ax,pad=0,ticks=[0.001,1],shrink=0.75)\n",
    "        cbar.ax.tick_params(labelsize=20)\n",
    "        cbar.ax.set_yticklabels(['\"Low fitness\"', '\"High fitness\"'])\n",
    "            \n",
    "        plt.title('%s to %s %s' %(pair[0],pair[1],year),style='italic',fontsize=40)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('C:/Users/Miles/Documents/Brown/Ogbunu Lab/Evo Word/Fitness/Figures/%s Letter Networks/%s to %s nx graph %s.png' %(str(WL),pair[0],pair[1],year),dpi=500)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
